#! /usr/local/bin/pythonw
#
# File: "Read Tweets Mutliline", version 2.0
# file name: readtweetsml_v2.py
#
# This file takes input in the form generated by quering the Twitter Search API for tweets
# messages containing a particular hashtag, along with the publishing date and author.
# "date","message","username" and converts it to a 1-mode digraph of Twitter users.
#
# HOW TO USE:
# This program accepts three arguments: the input filename, the output filename, and an
# optional time argument (integer)
# Example: python readtweetsml_v2.py inputfilenamehere.csv outputfilenamehere.txt 3
#
# INPUT:
# a csv file with three columns Date, Tweet content, username
# Example: "Fri, 08 Jan 2012, 13:22:45 +0000","RT@cnn @johndoe Protest in Egypt
# jan25","username@twitter.com (Authorname)"
# The "date" field is formatted as: DoW, dd MMM YYYY HH:MM:SS +0000
# +0000 refers to the timezone
# The message field can contain any characters allowed by Twitter, as well as Retweets and
# Tweet-ats and Hash-tags.
#
# OUTPUT:
# This script creates two files :
# First, edgelist of a 1-mode digraph of Twitter users, where a directed tie is assume to
# exist between two users if:
# 1. The author includes tweet-at to another user. A directed tie is create from the
# author to the mentioned user.
# 2. The author includes a retweet from the original author of a message. In this case, a
# directed tie is create from user mentioned in the re-tweet to the author.
# For example, the tweet "RT@cnn @johndoe Protest in Egypt #jan25" by user1 would create
# two ties:
# 1. From user1 to johndoe
# 2. From cnn to user1
#
# Second, if you specify the optional third argument, the script will generate a gexf formated
# file with timecodes.
#
# TIME:
# Call the script with the third argument (an integer) to use time values:
# "0" (zero) uses the time value provided by Twitter and has no decay ()
# Any integer greater than "0" (zero) X, defines the edge to exist from the start time
# provide by Twitter (t1) to t1 + X.
# For example, consider a tweet from user A to user B at 01:33:33 +0000:
# If you call the script with "3" as the value for the optional third argument,
# that tie will exist from A to B from 01:33:33 +0000 to 03:33:33 +0000.
#
# CHANGE LOG:
# Verion 1.0 to 1.2: 
# 1. Update to work with tweets which spanned multiple lines (e.g., contained newline
# characters).
# 2. Accepts timestamps as generated by the Twitter Search API
#
# Version 1.2 to 2.0:
# 1. Removes duplicate tweets (by indentifying identical entries, and only using one).
# 2. Out put in gexf format with optional time data.
#
# Released under Creative Commons (BY, NC, SA) by Russell Shepherd -
# russell.l.shepherd@gmail.com

import sys
import re
from datetime import datetime, timedelta

# Define our uniqify function for removing duplicate records
# from Peter Bengtsson, peterbe.com
def uniqify(seq, idfun=None): 
   # order preserving
   if idfun is None:
       def idfun(x): return x
   seen = {}
   result = []
   for item in seq:
       marker = idfun(item)
       # in old Python versions:
       # if seen.has_key(marker)
       # but in new ones:
       if marker in seen: continue
       seen[marker] = 1
       result.append(item)
   return result

# Get list index from value
def get_positions(xs, item):
    if isinstance(xs, list):
        for i, it in enumerate(xs):
            for pos in get_positions(it, item):
                yield (i,) + pos
    elif xs == item:
        yield ()

# Get file names 
f = open(sys.argv[1])
target = open(str(sys.argv[2]+'.txt'), "w")

# Check for decay time argument
if sys.argv[3]:
	decayTime = int(sys.argv[3])
	nodelist = []
	
# Read source file
unparsedTweets = f.read()

# Find individual tweets
newentry = re.compile(r'\n"[a-zA-Z]{3,3}.+?(?=\n"[a-zA-Z]{3,3}|\Z)', re.DOTALL)

# Load individual tweets into list parsedTweets
parsedTweets = newentry.findall(unparsedTweets)

# Find unique entries in list parsedTweets, load in uniqueTweets
uniqueTweets = uniqify(parsedTweets)

# Compile regex statements to find the date of message ("tweettime"), any retweets in message ("retweet"), any tweet-ats ("tweetat"), and the username of message author ("usr")

tweettime = re.compile(r'(?<="[a-zA-Z]{3,3},\s)\d{2,2}\s*[a-zA-Z]{3,3}\s\d{4,4}\s(\d{2,2}:){2,2}\d{2,2}\s(\+\d{4,4})(?=")', re.MULTILINE) # Timestamp (pubDate) regex statement
retweet = re.compile(r'(?<=RT\s@)[\w_]+(?=[\s:,])', re.IGNORECASE|re.MULTILINE)			# Retweet regex statement
tweetat = re.compile(r'(?<!RT\s@)(?<=@)[\w_]+(?=[\s:,])', re.IGNORECASE|re.MULTILINE)	# Tweet-at regex statement
usr = re.compile(r'(?<=,")[\w\W][^,]+(?=@twitter.com \()', re.IGNORECASE|re.MULTILINE)	# Username of author

# Now we write out edglist (no time argument)
# Loop through all parsed tweets, feed each tweet in "line"
for line in uniqueTweets:
	pubDate = tweettime.search(line) # Look for timestamp
	RT = retweet.search(line) 		 # Look for retweets
	AT = tweetat.findall(line)    	 # Look for tweet-ats
	SN = usr.search(line)            # Look for the username of the author	
	
	# If message is a retweet, create an edge leading from the original poster (mentioned in retweet) and the current author.
	if RT:
		toPrint = str(RT.group() + ',' + SN.group() + ',' + pubDate.group() + '\n')	# Create a string specifing a Retweet edge.
		toPrintStr = toPrint.lower() 		# Set all usernames to lowercase, as twitter is case-insensative.
		target.write(toPrintStr) 			# Print retweet edge to file.
		# Prepare a nodelist for graphml file if we were passed a time argument
		nodelist.append(SN.group())
		nodelist.append(RT.group())		

	# If the message contains tweet-ats, create a edge from the current author to each user tweeted at.
	if AT:
		for s in AT:
			toPrint = str(SN.group() + ',' + s + ',' + pubDate.group() +'\n')   # Create a string specifing a Tweet-at edge.
			toPrintStr = toPrint.lower()        # Set all usernames to lowercase.
			target.write(toPrintStr) 			# Print Tweet-at edge to file.
			nodelist.append(SN.group())
			nodelist.append(s)
    		
# Now the gexf file (if we were passed a time argument)

if decayTime:
	# remove duplicate entries from node list, save as an array (for "reverse" indexing)
	uniqueEdgelist = []
	uniqueNodelist = []
	
	for node in uniqify(nodelist):
		uniqueNodelist.append([node,])

	edgeID = 0
	# loop over tweets, create edgelist
	# format: <edge id="0" source="0" target="1" start="2009-03-01"/>
	for line in uniqueTweets:
		pubDate = tweettime.search(line) # Look for timestamp
		RT = retweet.search(line) 		 # Look for retweets
		AT = tweetat.findall(line)    	 # Look for tweet-ats
		SN = usr.search(line)            # Look for the username of the author

		# Convert timestamp to datetime object, create decay time.
		startTime = datetime.strptime(pubDate.group(), "%d %b %Y %H:%M:%S +0000")
		#GEXF accepts the XML dateTime object, formatted as: YYYY-MM-DDThh:mm:ss
		eTime = startTime + timedelta(hours=3)
		if decayTime > 0:
			endTime = str(' end=\"' + eTime.strftime("%Y-%m-%dT%H:%M:%S") + '\" ')
		
		
		# If the message contains a retweet tag:
		if RT:
			# Add the edge entry
			uniqueEdgelist.append(str('<edge id =\"' + str(edgeID) + '\" source=\"' + RT.group() + '\" target=\"' + SN.group() + '\" start=\"' + startTime.strftime("%Y-%m-%dT%H:%M:%S") + '\"' + endTime + '/>'))
			edgeID = edgeID + 1
			# Add a "spell" (see gexf format for more info) for corresponding nodes:
			uniqueNodelist[list(get_positions(uniqueNodelist, SN.group()))[0][0]].append(str('<spell start=\"' + startTime.strftime("%Y-%m-%dT%H:%M:%S") + '\"' + endTime + '/>'))
			uniqueNodelist[list(get_positions(uniqueNodelist, RT.group()))[0][0]].append(str('<spell start=\"' + startTime.strftime("%Y-%m-%dT%H:%M:%S") + '\"' + endTime + '/>'))
		
		# If the message contains tweet-at tags:
		if AT:
			for s in AT:
				uniqueEdgelist.append(str('<edge id =\"' + str(edgeID) + '\" source=\"' + SN.group() + '\" target=\"' + s + '\" start=\"' + startTime.strftime("%Y-%m-%dT%H:%M:%S") + '\"' + endTime + '/>'))
				edgeID = edgeID + 1
				# Add a "spell" (see gexf format for more info) for corresponding nodes:
				uniqueNodelist[list(get_positions(uniqueNodelist, SN.group()))[0][0]].append(str('<spell start=\"' + startTime.strftime("%Y-%m-%dT%H:%M:%S") + '\"' + endTime + '/>'))
				uniqueNodelist[list(get_positions(uniqueNodelist, s))[0][0]].append(str('<spell start=\"' + startTime.strftime("%Y-%m-%dT%H:%M:%S") + '\"' + endTime + '/>'))
	
	# Print results to file
	gexf = open(str(sys.argv[2]+'.gexf'), 'w')
	
	#GEXF header junk
	gexf.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n')
	gexf.write('<gexf xmlns=\"http://www.gexf.net/1.2draft\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.gexf.net/1.2draft http://www.gexf.net/1.2draft/gexf.xsd\" version=\"1.2\">\n')
	gexf.write('\t<graph mode=\"dynamic\" defaultedgetype=\"directed\" timeformat=\"dateTime\">\n')
	
	#Nodes
	gexf.write('\t\t<nodes>\n')
	for node in uniqueNodelist:
		gexf.write(str('\t\t\t<node id=\"' + node[0] + '\">\n'))
		gexf.write("\t\t\t\t<spells>\n")
		for spell in node[1:]:
			gexf.write(str("\t\t\t\t\t" + spell +"\n"))
		gexf.write("\t\t\t\t</spells>\n")
		gexf.write("\t\t\t</node>\n")
	gexf.write("\t\t</nodes>\n")
	
	#Edges
	gexf.write("\t\t<edges>\n")
	for edge in uniqueEdgelist:
		gexf.write(str("\t\t\t" + edge +"\n"))
	gexf.write("\t\t</edges>\n")
	
	#GEXF footer junk
	gexf.write("\t</graph>\n")
	gexf.write("</gexf>")
		